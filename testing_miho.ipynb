{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e8089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting open_spiel\n",
      "  Downloading open_spiel-1.6.3-cp39-cp39-macosx_11_0_arm64.whl (5.5 MB)\n",
      "     |████████████████████████████████| 5.5 MB 12.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.10.0 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from open_spiel) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from open_spiel) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.21.5 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from open_spiel) (2.0.2)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from open_spiel) (22.2.0)\n",
      "Requirement already satisfied: pip>=20.0.2 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from open_spiel) (21.3.1)\n",
      "Collecting ml-collections>=0.1.1\n",
      "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
      "     |████████████████████████████████| 77 kB 15.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from ml-collections>=0.1.1->open_spiel) (6.0)\n",
      "Requirement already satisfied: six in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from ml-collections>=0.1.1->open_spiel) (1.17.0)\n",
      "Requirement already satisfied: contextlib2 in /Users/mihokoda/opt/miniconda3/lib/python3.9/site-packages (from ml-collections>=0.1.1->open_spiel) (21.6.0)\n",
      "Building wheels for collected packages: ml-collections\n",
      "  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94563 sha256=4019fadf5fd4760fcff924bb221afbf8ea509c973d0972e04fa595b3d06345fe\n",
      "  Stored in directory: /Users/mihokoda/Library/Caches/pip/wheels/fd/c2/0d/5d94d95e5875ea17b85a9f1f99b8dd2e50517137c8042c6468\n",
      "Successfully built ml-collections\n",
      "Installing collected packages: ml-collections, open-spiel\n",
      "Successfully installed ml-collections-0.1.1 open-spiel-1.6.3\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/mihokoda/opt/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install open_spiel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9eeb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSpiel installed successfully!\n",
      "Number of games available: 113\n",
      "\n",
      "Loaded game: kuhn_poker\n",
      "Players: 2\n"
     ]
    }
   ],
   "source": [
    "import pyspiel\n",
    "\n",
    "# List available games\n",
    "print(\"OpenSpiel installed successfully!\")\n",
    "print(f\"Number of games available: {len(pyspiel.registered_games())}\")\n",
    "\n",
    "# Test loading Kuhn Poker\n",
    "game = pyspiel.load_game(\"kuhn_poker\")\n",
    "print(f\"\\nLoaded game: {game.get_type().short_name}\")\n",
    "print(f\"Players: {game.num_players()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c05377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: Terminal? false\n",
      "Row actions: Rock Paper Scissors \n",
      "Col actions: Rock Paper Scissors \n",
      "Utility matrix:\n",
      "0,0 -1,1 1,-1 \n",
      "1,-1 0,0 -1,1 \n",
      "-1,1 1,-1 0,0 \n",
      "\n",
      "Current player: -2\n",
      "Game over: True\n",
      "Returns: [1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "# Load RPS game\n",
    "game = pyspiel.load_game(\"matrix_rps\")\n",
    "state = game.new_initial_state()\n",
    "\n",
    "print(\"Initial state:\", state)\n",
    "print(\"Current player:\", state.current_player())  # Returns -2 for simultaneous\n",
    "\n",
    "# For simultaneous games, apply actions together\n",
    "state.apply_actions([0, 2])  # Player 0: Rock (0), Player 1: Scissors (2)\n",
    "\n",
    "print(\"Game over:\", state.is_terminal())\n",
    "print(\"Returns:\", state.returns())  # [1.0, -1.0] means P0 wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "000f96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: Kuhn Poker\n",
      "Players: 2\n",
      "Initial state: \n",
      "\n",
      "Game info:\n",
      "- Min utility: -2.0\n",
      "- Max utility: 2.0\n",
      "- Max game length: 3\n"
     ]
    }
   ],
   "source": [
    "# Load Kuhn Poker\n",
    "game = pyspiel.load_game(\"kuhn_poker\")\n",
    "state = game.new_initial_state()\n",
    "\n",
    "print(f\"Game: {game.get_type().long_name}\")\n",
    "print(f\"Players: {game.num_players()}\")\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# See information about the game\n",
    "print(\"\\nGame info:\")\n",
    "print(f\"- Min utility: {game.min_utility()}\")\n",
    "print(f\"- Max utility: {game.max_utility()}\")\n",
    "print(f\"- Max game length: {game.max_game_length()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facb117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COUNTERFACTUAL REGRET MINIMIZATION (CFR)\n",
      "Testing on Kuhn Poker (2-player zero-sum)\n",
      "============================================================\n",
      "\n",
      "=== CFR Solver Initialized ===\n",
      "Game: Kuhn Poker\n",
      "Players: 2\n",
      "Max actions: 2\n",
      "\n",
      "=== Starting CFR Training for 10000 iterations ===\n",
      "\n",
      "Iteration 1000/10000 - Avg game value: 0.000000\n",
      "Iteration 2000/10000 - Avg game value: 1.500000\n",
      "Iteration 3000/10000 - Avg game value: 2.000000\n",
      "Iteration 4000/10000 - Avg game value: 1.500000\n",
      "Iteration 5000/10000 - Avg game value: 0.000000\n",
      "Iteration 6000/10000 - Avg game value: 1.000000\n",
      "Iteration 7000/10000 - Avg game value: 1.500000\n",
      "Iteration 8000/10000 - Avg game value: -1.500000\n",
      "Iteration 9000/10000 - Avg game value: 0.000000\n",
      "Iteration 10000/10000 - Avg game value: 0.000000\n",
      "\n",
      "=== Training Complete ===\n",
      "Final average game values: [ 1. -1.]\n",
      "\n",
      "\n",
      "=== Average Strategy (Nash Equilibrium Approximation) ===\n",
      "\n",
      "Info Set: 0\n",
      "  Pass/Check: 0.9984 (99.84%)\n",
      "  Bet/Call: 0.0016 (0.16%)\n",
      "\n",
      "Info Set: 0b\n",
      "  Pass/Check: 0.9997 (99.97%)\n",
      "  Bet/Call: 0.0003 (0.03%)\n",
      "\n",
      "Info Set: 0p\n",
      "  Pass/Check: 0.9997 (99.97%)\n",
      "  Bet/Call: 0.0003 (0.03%)\n",
      "\n",
      "Info Set: 0pb\n",
      "  Pass/Check: 0.9999 (99.99%)\n",
      "  Bet/Call: 0.0001 (0.01%)\n",
      "\n",
      "Info Set: 1\n",
      "  Pass/Check: 0.0001 (0.01%)\n",
      "  Bet/Call: 0.9999 (99.99%)\n",
      "\n",
      "Info Set: 1b\n",
      "  Pass/Check: 0.0005 (0.05%)\n",
      "  Bet/Call: 0.9995 (99.95%)\n",
      "\n",
      "Info Set: 1p\n",
      "  Pass/Check: 0.9991 (99.91%)\n",
      "  Bet/Call: 0.0009 (0.09%)\n",
      "\n",
      "Info Set: 1pb\n",
      "  Pass/Check: 0.0001 (0.01%)\n",
      "  Bet/Call: 0.9999 (99.99%)\n",
      "\n",
      "Info Set: 2\n",
      "  Pass/Check: 0.0002 (0.02%)\n",
      "  Bet/Call: 0.9998 (99.98%)\n",
      "\n",
      "Info Set: 2b\n",
      "  Pass/Check: 0.0002 (0.02%)\n",
      "  Bet/Call: 0.9998 (99.98%)\n",
      "\n",
      "Info Set: 2p\n",
      "  Pass/Check: 0.0002 (0.02%)\n",
      "  Bet/Call: 0.9998 (99.98%)\n",
      "\n",
      "Info Set: 2pb\n",
      "  Pass/Check: 0.0002 (0.02%)\n",
      "  Bet/Call: 0.9998 (99.98%)\n",
      "\n",
      "=== Computing Exploitability (10000 samples) ===\n",
      "Average utility: [-0.0125  0.0125]\n",
      "Exploitability estimate: 0.012500\n",
      "~ Strategy is decent but could improve with more iterations\n",
      "\n",
      "============================================================\n",
      "VALIDATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "If exploitability is low (<0.01), CFR has converged to Nash!\n",
      "Next step: Test on 3-player games to see if it still works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class CFRSolver:\n",
    "    \"\"\"\n",
    "    Counterfactual Regret Minimization solver for 2-player zero-sum games.\n",
    "    \"\"\"\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.num_players = game.num_players()\n",
    "        \n",
    "        # Regret and strategy tables indexed by information set string\n",
    "        self.regret_sum = defaultdict(lambda: np.zeros(self.game.num_distinct_actions()))\n",
    "        self.strategy_sum = defaultdict(lambda: np.zeros(self.game.num_distinct_actions()))\n",
    "        self.info_set_map = {}  # Maps info state string to action list\n",
    "        \n",
    "        print(f\"=== CFR Solver Initialized ===\")\n",
    "        print(f\"Game: {game.get_type().long_name}\")\n",
    "        print(f\"Players: {self.num_players}\")\n",
    "        print(f\"Max actions: {game.num_distinct_actions()}\")\n",
    "        print()\n",
    "    \n",
    "    def train(self, num_iterations):\n",
    "        \"\"\"\n",
    "        Train CFR for a specified number of iterations.\n",
    "        \"\"\"\n",
    "        print(f\"=== Starting CFR Training for {num_iterations} iterations ===\\n\")\n",
    "        \n",
    "        util = np.zeros(self.num_players)\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            # Run CFR from the root for each player\n",
    "            for player in range(self.num_players):\n",
    "                state = self.game.new_initial_state()\n",
    "                util[player] = self.cfr(state, player, iteration)\n",
    "            \n",
    "            # Print progress every 10% of iterations\n",
    "            if (iteration + 1) % max(1, num_iterations // 10) == 0:\n",
    "                avg_game_value = np.mean(util)\n",
    "                print(f\"Iteration {iteration + 1}/{num_iterations} - Avg game value: {avg_game_value:.6f}\")\n",
    "        \n",
    "        print(f\"\\n=== Training Complete ===\")\n",
    "        print(f\"Final average game values: {util}\")\n",
    "        print()\n",
    "    \n",
    "    def cfr(self, state, player, iteration):\n",
    "        \"\"\"\n",
    "        Counterfactual Regret Minimization recursive algorithm.\n",
    "        Now handles both sequential and simultaneous games.\n",
    "        \"\"\"\n",
    "        # Terminal state - return utility\n",
    "        if state.is_terminal():\n",
    "            return state.returns()[player]\n",
    "        \n",
    "        # Chance node - sample and recurse\n",
    "        if state.is_chance_node():\n",
    "            outcomes = state.chance_outcomes()\n",
    "            action = np.random.choice([o[0] for o in outcomes], \n",
    "                                    p=[o[1] for o in outcomes])\n",
    "            state.apply_action(action)\n",
    "            return self.cfr(state, player, iteration)\n",
    "        \n",
    "        # HANDLE SIMULTANEOUS MOVES\n",
    "        if state.is_simultaneous_node():\n",
    "            return self.cfr_simultaneous(state, player, iteration)\n",
    "        \n",
    "        # Get current player (sequential game)\n",
    "        current_player = state.current_player()\n",
    "        \n",
    "        # Get information set (what the player knows)\n",
    "        info_state = state.information_state_string(current_player)\n",
    "        legal_actions = state.legal_actions()\n",
    "        num_actions = len(legal_actions)\n",
    "        \n",
    "        # Store legal actions for this info set\n",
    "        if info_state not in self.info_set_map:\n",
    "            self.info_set_map[info_state] = legal_actions\n",
    "        \n",
    "        # Get current strategy for this information set\n",
    "        strategy = self.get_strategy(info_state, legal_actions)\n",
    "        \n",
    "        # Compute counterfactual values for each action\n",
    "        action_values = np.zeros(num_actions)\n",
    "        \n",
    "        for i, action in enumerate(legal_actions):\n",
    "            new_state = state.child(action)\n",
    "            action_values[i] = self.cfr(new_state, player, iteration)\n",
    "        \n",
    "        # Compute expected value\n",
    "        expected_value = np.dot(strategy, action_values)\n",
    "        \n",
    "        # Update regrets only for the player we're computing CFR for\n",
    "        if current_player == player:\n",
    "            # Regret = value(action) - value(current strategy)\n",
    "            regrets = action_values - expected_value\n",
    "            \n",
    "            # Update cumulative regrets\n",
    "            for i, action in enumerate(legal_actions):\n",
    "                self.regret_sum[info_state][action] += regrets[i]\n",
    "        \n",
    "        # Update strategy sum for average strategy (for all players)\n",
    "        for i, action in enumerate(legal_actions):\n",
    "            self.strategy_sum[info_state][action] += strategy[i]\n",
    "        \n",
    "        return expected_value\n",
    "\n",
    "    def cfr_simultaneous(self, state, player, iteration):\n",
    "        \"\"\"\n",
    "        Handle simultaneous move nodes (like Rock-Paper-Scissors).\n",
    "        \"\"\"\n",
    "        # Get legal actions for all players\n",
    "        legal_actions_list = [state.legal_actions(p) for p in range(self.num_players)]\n",
    "        \n",
    "        # Get information sets and strategies for all players\n",
    "        strategies = []\n",
    "        info_states = []\n",
    "        \n",
    "        for p in range(self.num_players):\n",
    "            info_state = state.information_state_string(p)\n",
    "            info_states.append(info_state)\n",
    "            legal_actions = legal_actions_list[p]\n",
    "            \n",
    "            if info_state not in self.info_set_map:\n",
    "                self.info_set_map[info_state] = legal_actions\n",
    "            \n",
    "            strategy = self.get_strategy(info_state, legal_actions)\n",
    "            strategies.append(strategy)\n",
    "        \n",
    "        # Compute counterfactual values for the target player\n",
    "        target_legal_actions = legal_actions_list[player]\n",
    "        num_actions = len(target_legal_actions)\n",
    "        action_values = np.zeros(num_actions)\n",
    "        \n",
    "        # For each action of the target player\n",
    "        for i, action in enumerate(target_legal_actions):\n",
    "            value = 0.0\n",
    "            \n",
    "            # Sample opponent actions according to their strategies\n",
    "            opponent_actions = []\n",
    "            opponent_probs = 1.0\n",
    "            \n",
    "            for p in range(self.num_players):\n",
    "                if p == player:\n",
    "                    opponent_actions.append(action)\n",
    "                else:\n",
    "                    # Sample from opponent's strategy\n",
    "                    opp_legal_actions = legal_actions_list[p]\n",
    "                    opp_strategy = strategies[p]\n",
    "                    opp_action_idx = np.random.choice(len(opp_legal_actions), p=opp_strategy)\n",
    "                    opp_action = opp_legal_actions[opp_action_idx]\n",
    "                    opponent_actions.append(opp_action)\n",
    "                    opponent_probs *= opp_strategy[opp_action_idx]\n",
    "            \n",
    "            # Apply joint action\n",
    "            new_state = state.clone()\n",
    "            new_state.apply_actions(opponent_actions)\n",
    "            \n",
    "            # Recurse\n",
    "            action_values[i] = self.cfr(new_state, player, iteration)\n",
    "        \n",
    "        # Get expected value under current strategy\n",
    "        target_strategy = strategies[player]\n",
    "        expected_value = np.dot(target_strategy, action_values)\n",
    "        \n",
    "        # Update regrets for target player\n",
    "        info_state = info_states[player]\n",
    "        regrets = action_values - expected_value\n",
    "        \n",
    "        for i, action in enumerate(target_legal_actions):\n",
    "            self.regret_sum[info_state][action] += regrets[i]\n",
    "        \n",
    "        # Update strategy sum\n",
    "        for i, action in enumerate(target_legal_actions):\n",
    "            self.strategy_sum[info_state][action] += target_strategy[i]\n",
    "        \n",
    "        return expected_value\n",
    "    \n",
    "    def get_strategy(self, info_state, legal_actions):\n",
    "        \"\"\"\n",
    "        Get current strategy using Regret Matching.\n",
    "        Strategy is proportional to positive regrets.\n",
    "        \"\"\"\n",
    "        num_actions = len(legal_actions)\n",
    "        strategy = np.zeros(num_actions)\n",
    "        \n",
    "        # Get positive regrets\n",
    "        regrets = self.regret_sum[info_state]\n",
    "        positive_regrets = np.maximum(regrets[legal_actions], 0)\n",
    "        \n",
    "        sum_positive_regret = np.sum(positive_regrets)\n",
    "        \n",
    "        if sum_positive_regret > 0:\n",
    "            # Strategy proportional to positive regrets\n",
    "            strategy = positive_regrets / sum_positive_regret\n",
    "        else:\n",
    "            # Uniform random if no positive regrets\n",
    "            strategy = np.ones(num_actions) / num_actions\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def get_average_strategy(self, info_state=None):\n",
    "        \"\"\"\n",
    "        Get the average strategy (this converges to Nash equilibrium).\n",
    "        If info_state is None, return all average strategies.\n",
    "        \"\"\"\n",
    "        if info_state is not None:\n",
    "            # Return strategy for specific info state\n",
    "            legal_actions = self.info_set_map[info_state]\n",
    "            strategy_sum = self.strategy_sum[info_state][legal_actions]\n",
    "            \n",
    "            sum_strategy = np.sum(strategy_sum)\n",
    "            if sum_strategy > 0:\n",
    "                return strategy_sum / sum_strategy\n",
    "            else:\n",
    "                return np.ones(len(legal_actions)) / len(legal_actions)\n",
    "        else:\n",
    "            # Return all strategies\n",
    "            avg_strategies = {}\n",
    "            for info_state in self.info_set_map.keys():\n",
    "                avg_strategies[info_state] = self.get_average_strategy(info_state)\n",
    "            return avg_strategies\n",
    "    \n",
    "    def print_strategy(self, player=None):\n",
    "        \"\"\"\n",
    "        Print the learned average strategy in a readable format.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Average Strategy (Nash Equilibrium Approximation) ===\")\n",
    "        \n",
    "        action_names = {\n",
    "            0: \"Pass/Check\",\n",
    "            1: \"Bet/Call\"\n",
    "        }\n",
    "        \n",
    "        for info_state in sorted(self.info_set_map.keys()):\n",
    "            # Filter by player if specified\n",
    "            if player is not None and not info_state.startswith(str(player)):\n",
    "                continue\n",
    "                \n",
    "            legal_actions = self.info_set_map[info_state]\n",
    "            strategy = self.get_average_strategy(info_state)\n",
    "            \n",
    "            print(f\"\\nInfo Set: {info_state}\")\n",
    "            for i, action in enumerate(legal_actions):\n",
    "                action_name = action_names.get(action, f\"Action {action}\")\n",
    "                print(f\"  {action_name}: {strategy[i]:.4f} ({strategy[i]*100:.2f}%)\")\n",
    "\n",
    "\n",
    "def compute_exploitability(cfr_solver, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Compute exploitability: how much can a best-response opponent exploit our strategy?\n",
    "    Lower is better. 0 means Nash equilibrium.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Computing Exploitability ({num_samples} samples) ===\")\n",
    "    \n",
    "    game = cfr_solver.game\n",
    "    total_utility = np.zeros(cfr_solver.num_players)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        state = game.new_initial_state()\n",
    "        \n",
    "        while not state.is_terminal():\n",
    "            if state.is_chance_node():\n",
    "                outcomes = state.chance_outcomes()\n",
    "                action = np.random.choice([o[0] for o in outcomes], \n",
    "                                         p=[o[1] for o in outcomes])\n",
    "                state.apply_action(action)\n",
    "            \n",
    "            # HANDLE SIMULTANEOUS MOVES\n",
    "            elif state.is_simultaneous_node():\n",
    "                joint_action = []\n",
    "                for p in range(cfr_solver.num_players):\n",
    "                    info_state = state.information_state_string(p)\n",
    "                    legal_actions = state.legal_actions(p)\n",
    "                    \n",
    "                    # Use average strategy\n",
    "                    if info_state in cfr_solver.info_set_map:\n",
    "                        strategy = cfr_solver.get_average_strategy(info_state)\n",
    "                        action_idx = np.random.choice(len(legal_actions), p=strategy)\n",
    "                        action = legal_actions[action_idx]\n",
    "                    else:\n",
    "                        action = np.random.choice(legal_actions)\n",
    "                    \n",
    "                    joint_action.append(action)\n",
    "                \n",
    "                state.apply_actions(joint_action)\n",
    "            \n",
    "            # SEQUENTIAL MOVES\n",
    "            else:\n",
    "                current_player = state.current_player()\n",
    "                info_state = state.information_state_string(current_player)\n",
    "                legal_actions = state.legal_actions()\n",
    "                \n",
    "                # Use average strategy\n",
    "                if info_state in cfr_solver.info_set_map:\n",
    "                    strategy = cfr_solver.get_average_strategy(info_state)\n",
    "                    action_idx = np.random.choice(len(legal_actions), p=strategy)\n",
    "                    action = legal_actions[action_idx]\n",
    "                else:\n",
    "                    action = np.random.choice(legal_actions)\n",
    "                \n",
    "                state.apply_action(action)\n",
    "        \n",
    "        returns = state.returns()\n",
    "        total_utility += returns\n",
    "    \n",
    "    avg_utility = total_utility / num_samples\n",
    "    exploitability = np.max(np.abs(avg_utility))\n",
    "    \n",
    "    print(f\"Average utility: {avg_utility}\")\n",
    "    print(f\"Exploitability estimate: {exploitability:.6f}\")\n",
    "    \n",
    "    if exploitability < 0.01:\n",
    "        print(\"✓ Strategy appears to be near Nash equilibrium!\")\n",
    "    elif exploitability < 0.1:\n",
    "        print(\"~ Strategy is decent but could improve with more iterations\")\n",
    "    else:\n",
    "        print(\"✗ Strategy is highly exploitable, needs more training\")\n",
    "    \n",
    "    return exploitability\n",
    "\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"COUNTERFACTUAL REGRET MINIMIZATION (CFR)\")\n",
    "    print(\"Testing on Kuhn Poker (2-player zero-sum)\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Load Kuhn Poker\n",
    "    game = pyspiel.load_game(\"kuhn_poker\")\n",
    "    \n",
    "    # Create CFR solver\n",
    "    cfr = CFRSolver(game)\n",
    "    \n",
    "    # Train CFR\n",
    "    NUM_ITERATIONS = 10000\n",
    "    cfr.train(NUM_ITERATIONS)\n",
    "    \n",
    "    # Print learned strategy\n",
    "    cfr.print_strategy()\n",
    "    \n",
    "    # Compute exploitability\n",
    "    exploitability = compute_exploitability(cfr, num_samples=10000)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VALIDATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nIf exploitability is low (<0.01), CFR has converged to Nash!\")\n",
    "    print(f\"Next step: Test on 3-player games to see if it still works.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
